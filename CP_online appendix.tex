 \documentclass[12pt]{ectaart}
 \usepackage[utf8]{inputenc}
 %\usepackage[english]{babel}
 \sloppy
 \renewcommand{\baselinestretch}{1}
 %\usepackage{epsfig}
 
 \usepackage{graphics}
 \usepackage{amsmath, amssymb, amsthm}
 \usepackage{graphicx}
 \usepackage{verbatim}
 \usepackage{amsfonts}
 %font
 \usepackage[T1]{fontenc}
 \usepackage{mathpazo}
 \usepackage{fancyvrb}
 \usepackage{color}
 \usepackage{mdwlist}
 
 
 \usepackage{hyperref}
 
 
 
 
 % caligraphic
 \usepackage{mathrsfs}
 %\usepackage{bbm}
 
 %% page layout
 \usepackage[left=1.5in, right=1.5in, top=1in, bottom=1in, includehead, includefoot]{geometry}
 %\usepackage{lipsum}
 %\usepackage{titlesec}
 
 %\titlespacing\section{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
 %\titlespacing\subsection{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
 %\titlespacing\subsubsection{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
 
 
 %\usepackage[linesnumbered, ruled, lined]{algorithm2e}
 %\usepackage[linesnumbered, lined]{algorithm2e}
 
 
 %put notes at the end
 %\usepackage{endnotes}
 %\let\footnote=\endnote
 
 %extra spacing
 
 %horizonal line
 \newcommand{\HRule}{\rule{\linewidth}{0.3mm}}
 
 % skip a line between paragraphs, no indentation
 \setlength{\parskip}{1.5ex plus0.5ex minus0.5ex}
 \setlength{\parindent}{0pt}
 
 \DeclareMathOperator{\cl}{cl}
 \DeclareMathOperator*{\argmax}{\arg\,max}
 %\DeclareMathOperator{\overset{\circ}}{int}
 \DeclareMathOperator{\Prob}{Prob}
 \DeclareMathOperator{\determinant}{det}
 \DeclareMathOperator{\Var}{Var}
 \DeclareMathOperator{\Cov}{Cov}
 \DeclareMathOperator{\graph}{graph}
 \DeclareMathOperator{\Gr}{Gr}
 \DeclareMathOperator{\inte}{int}
 \DeclareMathOperator{\di}{d}
 \DeclareMathOperator{\dom}{dom}
 \DeclareMathOperator{\ran}{range}
 
 
 % mics short cuts and symbols
 \newcommand{\st}{\ensuremath{\ \mathrm{s.t.}\ }}
 \newcommand{\setntn}[2]{ \{ #1 : #2 \} }
 \newcommand{\fore}{\therefore \quad}
 \newcommand{\toas}{\stackrel {\textrm{ \scriptsize{a.s.} }} {\to} }
 \newcommand{\tod}{\stackrel { d } {\to} }
 \newcommand{\towa}{\stackrel[^Q]{ w.a. } {\longrightarrow} }
 \newcommand{\toua}{\stackrel[^Q] { u.a. } {\longrightarrow} }
 \newcommand{\tosa}{\stackrel[^Q] { s.a. } {\longrightarrow} }
 \newcommand{\tota}{\stackrel[^Q] { t.a. } {\longrightarrow} }
 \newcommand{\towap}{\stackrel[^{Q \times Q}]{ w.a. } {\longrightarrow} }
 \newcommand{\touap}{\stackrel[^{Q \times Q}] { u.a. } {\longrightarrow} }
 \newcommand{\tosap}{\stackrel[^{Q \times Q}] { s.a. } {\longrightarrow} }
 \newcommand{\totap}{\stackrel[^{Q \times Q}] { t.a. } {\longrightarrow} }
 \newcommand{\disteq}{\stackrel { \mathscr D } {=} }
 \newcommand{\eqdist}{\stackrel {\textrm{ \scriptsize{d} }} {=} }
 \newcommand{\iidsim}{\stackrel {\textrm{ {\sc iid }}} {\sim} }
 \newcommand{\1}{\mathbbm 1}
 \newcommand{\la}{\langle}
 \newcommand{\ra}{\rangle}
 \newcommand{\dee}{\,{\rm d}}
 \newcommand{\og}{{\mathbbm G}}
 \newcommand{\ctimes}{\! \times \!}
 \newcommand{\sint}{{\textstyle\int}}
 
 \newcommand{\A}{\forall}
 
 
 \makeatletter
 \def\munderbar#1{\underline{\sbox\tw@{$#1$}\dp\tw@\z@\box\tw@}}
 \makeatother
 
 %\renewcommand{\times}{\! \times \!}
 
 \newcommand{\aA}{\mathscr A}
 \newcommand{\cC}{\mathscr C}
 \newcommand{\dD}{\mathscr D}
 \newcommand{\sS}{\mathscr S}
 \newcommand{\bB}{\mathscr B}
 \newcommand{\gG}{\mathscr G}
 \newcommand{\hH}{\mathscr H}
 \newcommand{\kK}{\mathscr K}
 \newcommand{\lL}{\mathscr L}
 \newcommand{\mM}{\mathscr M}
 \newcommand{\eE}{\mathscr E}
 \newcommand{\fF}{\mathscr F}
 \newcommand{\qQ}{\mathscr Q}
 \newcommand{\xX}{\mathscr X}
 \newcommand{\zZ}{\mathscr Z}
 \newcommand{\wW}{\mathscr W}
 \newcommand{\uU}{\mathscr U}
 
 \newcommand{\pP}{\mathscr P}
 
 \newcommand{\vV}{\mathcal V}
 
 \newcommand{\RR}{\mathbbm R}
 \newcommand{\QQ}{\mathbbm Q}
 \newcommand{\NN}{\mathbbm N}
 \newcommand{\PP}{\mathbbm P}
 \newcommand{\EE}{\mathbbm E \,}
 \newcommand{\WW}{\mathbbm W}
 \newcommand{\UU}{\mathbbm U}
 \newcommand{\ZZ}{\mathbbm Z}
 
 \newcommand{\var}{\mathbbm V}
 
 
 \newcommand{\XX}{\mathcal X}
 
 \newcommand{\bP}{\mathbf P}
 \newcommand{\bQ}{\mathbf Q}
 \newcommand{\bE}{\mathbf E}
 \newcommand{\bM}{\mathbf M}
 \newcommand{\bX}{\mathbf X}
 \newcommand{\bY}{\mathbf Y}
 \theoremstyle{plain}
 \newtheorem{axiom}{Axiom}[section]
 \newtheorem{theorem}{Theorem}[section]
 \newtheorem{corollary}{Corollary}[section]
 \newtheorem{lemma}{Lemma}[section]
 \newtheorem{proposition}{Proposition}[section]
 \newtheorem{claim}{Claim}[section]
 \newtheorem{fact}{Fact}[section]
 
 
 \theoremstyle{definition}
 \newtheorem{definition}{Definition}[section]
 \newtheorem{example}{Example}[section]
 \newtheorem{remark}{Remark}[section]
 \newtheorem{notation}{Notation}[section]
 \newtheorem{assumption}{Assumption}[section]
 \newtheorem{condition}{Condition}[section]
 
 \usepackage{tikz}
 \usepackage{pgfplots}
 \pgfplotsset{compat=1.9, 
 	standard/.style={separate axis lines,
 		axis lines=middle, axis line style={-,big arrow} % I WANT "big arrow" HERE
 	}
 }
 \usetikzlibrary{datavisualization}
 \usetikzlibrary{datavisualization.formats.functions}
 \usetikzlibrary{arrows,decorations.markings}
 
 \usetikzlibrary{arrows.meta}
 
 \usepackage{xr}
 \externaldocument[AI-]{ExistenceCPAiyagari_ECTA_v5}
 
 %usepackage{xr}
 %\externaldocument[ET-]{existence_onlineappx}
 %%%%%%%%%%%%%%%%%% end my preamble %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \usepackage[round]{natbib}
 \usepackage{authblk}
 \bibliographystyle{elsarticle-harv}
 
 \usepackage{enumitem}
 \usepackage{dsfont}
 \raggedbottom 
 
 \usepackage{array}
 \newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
 \newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
 \newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
 
 \makeatletter
 \newcommand{\proofpart}[2]{%
 	\par
 	\addvspace{\medskipamount}%
 	\noindent\emph{Part #1: #2}\par\nobreak
 	\addvspace{\smallskipamount}%
 	\@afterheading
 }
 \makeatother
 
 \makeatletter
 \let\@fnsymbol\@arabic
 \makeatother
 
 
 \setcounter{section}{+7}
 
 \begin{document}
 	
 	
 	
 	
 	
 	
 	\title{Online Appendix for Existence of Constrained Optimal Policies in the Heterogeneous Agent Growth Model}
 	\runtitle{Appendix: Existence of Constrained Optima}
 	
 	
 	\author{Akshay Shanker\thanks{Please downloand the latest version of this appendix at \url{https://github.com/mathuranand/Existence_of_Social_Optimia_Aiyagari}
 	}}
 	
 	
 	\affil{Australian National University (ANU)}
 	
 	
 	
 	
 	
 	
 	\maketitle
 	
\section{Mathematical Preliminaries}\label{sec: appexa}


\subsection{Correspondences}\label{sec: corr}

Let $(X,\tau)$ and $(Y, \tau^{\prime})$ be topological vector spaces. A correspondence from a space $X$ to $Y$ is a set valued function denoted by $\Gamma\colon X\twoheadrightarrow Y$. The image of a subset $A$ of $X$ under the  correspondence $\Gamma$ will be the set \[\Gamma\left(A\right)\colon=\left\{ y\in Y\vert y\in\Gamma\left(x\right)\,\text{for some}\,x\in A\right\}\]A correspondence will be called \textbf{compact valued} if $\Gamma(x)$ is compact for $x\in X$. We can also define the graph of a correspondence as
$\Gr\Gamma\colon= \{(x,y)\vert y\in \Gamma(x)\}$. A correspondence will have a \textbf{closed graph} if $\Gr\Gamma$ is closed. 


A correspondence is \textbf{upper hemi-continuous} if for every $x$ and neighbourhood $U$ of $\Gamma(x)$, there is a neighbourhood $V$ of $x$ such that $z\in V$ implies $\Gamma(z)\subset U$. A correspondence is \textbf{lower hemi-continuous} if at each x, for every open set $U$ such that $\Gamma(x)\cap U \not= \emptyset$ there is a neighbourhood $V$ of $x$ such that for any $z\in V$ we have $\Gamma(z)\cap U \not= \emptyset$.  

Upper hemicontinuous correspondences need not be compact valued or have closed graph. Closed graph correspondences also need not be upper hemi-continuous. However, 

\begin{lemma}
	If $\Gamma\colon X\twoheadrightarrow Y$ is upper hemicontinuous and compact valued, then for $C\subset X$ such that $C$ is compact, $\Gamma(C)$ is compact. 
\end{lemma}
	
	See Lemma 17.8  \cite{Aliprantis2005}) for a proof. 

\subsection{Semicontinuity}\label{sec: semicont}

A function $f\colon X\rightarrow \bar{\mathbb{R}}$ is  sequentially \textbf{upper semi-continuous} if the upper contour sets 
%
\begin{equation}
UC_f(\epsilon)\colon= \left\{x\in X\,\vert\, f\left(x\right) \geq \epsilon \right\}
\end{equation}
%
are sequentially closed for all $\epsilon\in \mathbb{R}$. When writing $UC_{f}(\epsilon)$, I will omit the term $\epsilon$ if the choice of $\epsilon$ is clear.

Equivalently, $f$ is sequentially upper semi-continuous if for $x_{n}\rightarrow x$ with $x\in X$, we have $\limsup\limits_{n\rightarrow \infty} f(x_{n}) \leq f(x)$. 
% check definition of USC%

% What happens when the domain of U is a subset of L2%
Let $D$ be a subset of $\bar{\mathbb{R}}$. A function $f\colon X \rightarrow D$ will be called  \textbf{sup-compact} if the sets $UC_{f}(\epsilon)$ are  sequentially compact for all $\epsilon\in \mathbb{R}$. 

If $X$ is not compact and $D$ is bounded below, then $f$ cannot be sup-compact; we will need a weaker condition for bounded functions that captures the same idea as sup-compactness.  Mild sup-compactness generalises sup-compactness by only requiring upper contour sets away from the infimum to be sequentially compact. Suppose $D \subset \mathbb{R}_{+}$, then the function $f$ will be called \textbf{mildly sup-compact} if the sets $UC_{f}(\epsilon)$ are  sequentially compact for all $\epsilon > 0$. 

%I will drop the prefix sequentially in this paper, and upper semi-continuity and sup-compact should be understood to imply sequentially closed and sequentially compact upper countour sets respectively. 
\begin{figure}[h]
	\begin{tikzpicture}[baseline=(current bounding box.north)]
	\begin{axis}[
	axis y line = middle,
	axis x line = middle,
	width=0.5\textwidth,
	title style={at={(0.5,-0.2)},text width=.5\textwidth,anchor=north,align=center},
	title={Mildly Sup-Compact}, ticks = none, axis line style={-Latex[round]}
	]
	\addplot [mark=none] {exp(x)+1}
	node [pos=0.2, above left] {$y=e^x$};
	\addplot [draw=white,fill=white]
	coordinates {(4,3) (4,150) (6,150) (6,3)};
	\addplot [draw=black]
	coordinates {(3.5,-10) (4,-10) (4,10) (3.5,10)};
	\addplot [draw= black](4,53) node[anchor=south, fill, circle, inner sep=0pt,minimum size=4pt] {.};
	\end{axis}
	\end{tikzpicture}
	\begin{tikzpicture}[baseline=(current bounding box.north)]
	\begin{axis}[
	axis y line = middle,
	axis x line = middle,
	width=0.5\textwidth,
	title style={at={(0.5,-0.2)},text width=.5\textwidth,anchor=north,align=center},
	title={Sup-Compact}, ticks = none, axis line style={-Latex[round]}
	]
	\addplot [mark=none] {x^3 - x}
	node [pos=0.5, above left] {$y=x^3 -x$};
	\addplot [draw=white,fill=white]
	coordinates {(4,3) (4,150) (6,150) (6,3)};
	\addplot [draw=black]
	coordinates {(3.5,-15) (4,-15) (4,15) (3.5,15)};
	\addplot [draw= black](4,53) node[anchor=south, fill, circle, inner sep=0pt,minimum size=4pt] {.};
	
	\end{axis}
	\end{tikzpicture}
	\caption{Not all upper-contour sets of the the mildly sup-compact function are compact since it is bounded below, but the function still has a maximum.}
	\label{fig: SC}
\end{figure}

\subsection{Probability and Conditional Expectation}\label{sec: probcond}
Let $(\Omega, \Sigma, \mathbb{P})$ be a probability space. We work with the following definition of conditional expectation. 

\begin{definition}
	Let $\mathcal{H}\subset \Sigma$ be a sub -$\sigma$ -algebra of $\Sigma$ and let $x\colon \Omega\rightarrow \mathbb{R}^{n}$ be a random variable. The \textbf{conditional expectation} of $x$ given $\mathcal{H}$ is any $\mathcal{H}$-measurable random variable $y$ which satisfies
	\[
	\int_{B}y\,\mathbb{P} = \int_{B}x\,d\mathbb{P},\qquad B\in \mathcal{H}\]
\end{definition}

If $y$ is a conditional expectation of $x$ given $\mathcal{H}$, we write $ y= \mathbb{E}(x\vert \mathcal{H})$.


Recall the definition of a $\sigma$-algebra generated by a family of functions $\{y_{i}\}_{i\in F}$: \[\sigma(\{y_{i}\}_{i\in F}) \colon = \sigma(\cup_{i\in F}\sigma(y_{i}))\]

Recall also that if  $\mathcal{A}$ and $\mathcal{B}$ are independent sub $\sigma$-algebras of $\Sigma$ and if a sub-$\sigma$-algebra $\mathcal{C}$ satisfies $\mathcal{C}\subset \mathcal{A}$, then $\mathcal{C}$ and $\mathcal{B}$ will be independent.

%We will also use the notation $\sigma(\mathcal{H},\mathcal{G})$ to mean $\sigma(\mathcal{G}\cup \mathcal{H})$, where $\mathcal{G}$ and $\mathcal{H}$ are sub $\sigma$-algebras of $\Sigma$.

The following facts are standard. 

\begin{fact}\label{fact: indepex}
	Let $\mathcal{G}$ and $\mathcal{H}$ be sub $\sigma$-algebras of $\Sigma$ and let $x\colon \Omega \rightarrow \mathbb{R}$ be a random variable. If $\mathcal{H}$ and $\sigma(\mathcal{G},\sigma(x))$ are independent, then $\mathbb{E}(x\vert \sigma(\mathcal{G},\mathcal{H}))= \mathbb{E}(x\vert \mathcal{G})$.
\end{fact}
\begin{proof}
	See section 9.7 in \cite{Williams1991}.
\end{proof}

\begin{fact}\label{clm: sigmaalg1}
	If $\mathcal{A},\mathcal{B}$ and $\mathcal{C}$ are $\sigma$-algebras, then $\sigma(\mathcal{A}\cup \mathcal{B})\cup \mathcal{C} \subset \sigma(\mathcal{A}\cup \mathcal{B}\cup \mathcal{C})$
\end{fact}

\begin{proof}
	
	Recall that for collections of sets $\mathcal{A}$ and $\mathcal{B}$, with $\mathcal{A}\subset \mathcal{B}$, we have $\sigma(\mathcal{A})\subset \sigma(\mathcal{B})$. 
	
	Now pick any $B$ such that $B\in \sigma(\mathcal{A}\cup \mathcal{B})\cup \mathcal{C}$. We have either $B\in \sigma(\mathcal{A}\cup \mathcal{B})$ or $B\in \mathcal{C}$. If $B\in \sigma(\mathcal{A}\cup \mathcal{B})$, then since $\sigma(\mathcal{A}\cup \mathcal{B})\subset \sigma(\mathcal{A}\cup \mathcal{B}\cup \mathcal{C})$, $B\in \sigma(\mathcal{A}\cup \mathcal{B}\cup\mathcal{C})$. Alternatively, if $B\in \mathcal{C}$, then because $\mathcal{C}\subset \mathcal{A}\cup\mathcal{B}\cup \mathcal{C}\subset \sigma(\mathcal{A}\cup\mathcal{B}\cup\mathcal{C})$, we can conclude that $B\in \sigma(\mathcal{A}\cup \mathcal{B}\cup\mathcal{C})$. 
\end{proof}


\begin{fact}(\textbf{Doob-Dynkin})\label{fact: dd}
	Let $(\Omega, \Sigma, \mathbb{P})$ be a probability space. Let $f\colon \Omega\rightarrow \mathbb{R}^{k}$ and $g\colon \Omega \rightarrow \mathbb{R}^{n}$. The generated $\sigma$-algebras satisfy $\sigma(f)\subset \sigma(g)$ if and only if there exists a measurable function $h\colon \mathbb{R}^{k}\rightarrow \mathbb{R}^{n}$ such that $f = h\circ g$.
\end{fact}
\begin{proof}
	See Lemma 1.13 by \cite{Kallenberg1997}.
\end{proof}

\begin{fact}(\textbf{Jensen's Inequality})\label{fact: jensen}
	
	Let $x$ be a random variable on a probability space $(\Omega, \Sigma, \mathbb{P})$. If  $c \colon \mathbb{R} \rightarrow \mathbb{R}$ is convex, and $\mathbb{E}\vert c(x)\vert <\infty$, then 
	%
	\begin{equation*}
	\mathbb{E}(c(x)\vert \mathcal{H}) \geq c(\mathbb{E}(x\vert \mathcal{H})) 
	\end{equation*}
	%
	almost everywhere, for any sub $\sigma$ - algebra $\mathcal{H}$
\end{fact}

For a proof, See section 9.7 in \cite{Williams1991}.


\begin{fact}(\textbf{Reverse Fatou's Lemma})\label{fact: fatou}
		 If $(f_{n})$  is a sequence of real valued functions defined on a measure space $(\Omega, \Sigma, \mu)$, and each $f_{n}$ satisfies $f_{n}\leq g$, where $g$ is an integrable function on $(\Omega, \Sigma, \mu)$, then $$\limsup_{n\rightarrow \infty}\int f_{n}d\mu \leq \int \limsup_{n\rightarrow\infty}f_{n}d\mu$$
\end{fact}

For a proof, see 5.4 in \cite{Williams1991}.

	\subsection*{Markov Properties}\label{sec: markov}
	
	Let $S\subset \mathbb{R}$. Following \cite{Stachurski2009}, section 9.2, we can characterise any $S$- valued Markov process $(e_{t})_{t=0}^{\infty}$ on a probability space $(\Omega,\Sigma , \mathbb{P})$ recursively. In particular, let $G\colon S\times \Omega  \rightarrow S$, we will have
	%
	\begin{equation}\label{eq: SRS}
	e_{t+1} = G(e_{t},\eta_{t+1}), \qquad t\in \mathbb{N}
	\end{equation}
	%
	where $(\eta_{t})_{t=1}^{\infty}$ is an {\sc i.i.d}. sequence of random variables defined on $(\Omega,\Sigma , \mathbb{P})$ and $e_{0}$ is given. 
	
	The dynamics of an $S$-valued Markov process on $(Z,\mathscr{F}, P)$ can be summarised by a stochastic kernel $Q$. The value $Q(x,B)$ represents the probability that the Markov process moves from $x$ to $B$,  with $x\in S$ and $B\in \mathscr{B}(S)$ in a unit of time. The Markov kernel satisfies $Q(x, \cdot) \in \mathscr{P}({S})$ for each $x\in S$, where  $\mathscr{P}({S})$ is the space of probability measures on $S$. Moreover, $Q(\cdot, B)$ is measurable for each $B$. 
	
	Letting $\eta_{t}\sim \phi$ for each $t$, we can relate the recursive characterisation of the Markov process to the stochastic kernel as follows
	%
	\begin{equation}\label{eq: kersrs}
	Q(x,B) = \mathbb{P}\{G(x,\eta_{t+1})\in B\} = \mathbb{E}\mathbb{1}_{B}\{G(x,\eta_{t+1})\} = \int \mathbb{1}_{B}\{G(x,z)\}\phi(dz)
	\end{equation}
	%
	for $x\in S$ and $B\in \mathscr{B}(S)$.

	   
	   
	   

\section{Proofs}


	\subsection*{Proofs for Section \ref{AI-sec: Aiyagari}}
	
	Consider the setting described in section \ref{AI-sec: Aiyagari}. The following claim defines the  distributions of $(x_{t}^{i})_{t=0}^{\infty}$ and $(e_{t}^{i})_{t=0}^{\infty}$ under a sequence of policy functions $(h_{t})_{t=0}^{\infty}$. 
	
	
	\begin{claim}\label{clm: recmeas}
		Let Assumptions \ref{AI-ass: shocks1} and \ref{AI-ass: shocks1} hold. If $(x_{t}^{i})_{t=0}^{\infty}$ is defined by the recursion in Equation \eqref{AI-eq: nxtasset} of the main paper for each $i\in [0,1]$, then for each $t\in \mathbb{N}$ and $i\in [0,1]$, $\{x_{t+1}^{i}, e_{t+1}^{i}\} \sim \mu_{t+1}$ and $\{x_{t}^{i}, e_{t}^{i}\} \sim \mu_{t}$ where $\mu_{t+1}$ and $\mu_{t}$ satisfy the recursion \eqref{AI-eq: mutdef} of the main paper.
	\end{claim}
	
	\begin{proof} 
		
		We will proceed inductively and first show the claim holds for $t=0$. By Assumption \ref{AI-ass: shocks1}, let $\mu_{0}$ be given as the joint distribution of $x_{0}^{i}$ and $e_{0}^{i}$. We then have $\{x_{0}^{i}, e_{0}^{i}\}\sim \mu_{0}$ for each $i$. Moreover, the joint distribution of $x_{1}^{i}$ and $e_{1}^{i}$ will be
		%
		\begin{align*}
		\mu_{1}(B_{A}\times B_{E}) \colon & = \mathbb{P}\left\{x_{1}^{i}\in B_{A}, e_{1}^{i}\in B_{E}\right\}\\
		& = \int \mathbb{1}_{B_{A}}\{x_{1}^{i}\}\times\mathbb{1}_{B_{E}}\{e_{1}^{i}\}\,d\mathbb{P}\\
		& = \int\int \int \mathbb{1}_{B_{A}}\left\{h_{0}(x, e)\right\}\\
		& \quad \times \mathbb{1}_{B}\left\{G(e, \eta)\right\}\mu_{0}(dx, de)\phi(d\eta)\\
		& = \int\int \mathbb{1}_{B_{A}}\left\{h_{0}(x, e)\right\}\\
		& \quad \times \left[\int \mathbb{1}_{B_{E}} \left\{G(e, \eta)\right\}\phi(d\eta)\right]\mu_{0}(dx, de)\\
		& = \int\int \mathbb{1}_{B_{A}}\left\{h_{0}(x, e)\right\} Q(e,B_{E})\mu_{0}(dx, de)
		\end{align*}
		%
		The first equality is given by the standard definition of expectations. The second equality follows from Equation \eqref{AI-eq: nxtasset}, the recursive characterisation of the Markov process (Equation \eqref{eq: SRS} above) and because $\mu_{0}$ is the marginal distribution of $\{x_{0}^{i},e_{0}^{i}\}$ and $\phi$ is the marginal distribution of the {\sc iid} shock $\eta^{i}_{t}$. The final line follows from the properties of  Markov kernel, in particular, Equation \eqref{eq: kersrs} above. 
		
		The above argument shows $\mu_{1}$ and $\mu_{0}$ satisfy the recursion \eqref{AI-eq: mutdef} and hence the claim holds for $t=0$. 
		
		Now make the inductive assumption that the claim holds for arbitrary $t$, that is,  $\{x_{t+1}^{i}, e_{t+1}^{i}\}\sim \mu_{t+1}$ and $\{x_{t}^{i}, e_{t}^{i}\}\sim \mu_{t}$ where $\mu_{t}$ and $\mu_{t+1 }$ satisfy the recursion \eqref{AI-eq: mutdef} for each $i$. To see the claim holds for $t+1$, 
		%
		\begin{align*}
		\mu_{t+2}(B_{A}\times B_{E}) \colon & = \mathbb{P}\left\{x_{t+2}^{i}\in B_{A}, e_{t+2}^{i}\in B_{E}\right\}\\
		& = \int \mathbb{1}_{B_{A}}\{x_{t+2}^{i}\}\times\mathbb{1}_{B_{E}}\{e_{t+2}^{i}\}\,d\mathbb{P}\\
		& = \int\int \int \mathbb{1}_{B_{A}}\left\{h_{t+1}(x, e)\right\}\\
		& \quad \times \mathbb{1}_{B_{E}}\left\{G(e, \eta)\right\}\mu_{t+1}(dx, de)\phi(d\eta)\\
		& = \int\int \mathbb{1}_{B_{A}}\left\{h_{t+1}(x, e)\right\}\\
		& \quad\times \left[\int \mathbb{1}_{B_{E}} \left\{G(e, \eta)\right\}\phi(d\eta)\right]\mu_{t+1}(dx, de)\\
		& = \int\int \mathbb{1}_{B_{A}}\left\{h_{t+1}(x, e)\right\} Q(e,B_{E})\mu_{t+1}(dx, de)
		\end{align*}
		
	\end{proof}
	
	
	For the following claim, consider the setting of section \ref{AI-sec: Aiyagari} and let $(h_{t})_{t=0}^{\infty}$ be a sequence of measurable functions with $h_{t}\colon S\rightarrow A$ for each $t$.  Let $(x_{t}^{i})_{t=0}^{\infty}$ be a sequence of random variables generated by Equation \eqref{AI-eq: nxtasset} for each $t$. Let $(x_{t}^{i})_{t=0}^{\infty}$ satisfy Equation \eqref{AI-eq: agentBC} for each $t$. Recall by Claim \ref{clm: recmeas} that $\left\{x_{t}^{i},e_{t}^{i}\right\}\sim \mu_{t}$ for each $t$.
	
	\begin{claim}\label{clm: finvar}
		If Assumption \ref{AI-ass: shocks1} holds  and $r(\mu_{t})< \infty$ for each $t$, then $x_{t}^{i}$ has finite variance for each $t$.
	\end{claim}
	\begin{proof}
		
		We will proceed by induction. First we confirm that if $x_{t}^{i}$ has finite variance for some $t$, then $x_{t+1}^{i}$ will have finite variance. Since $x_{t+1}^{i}$ satisfies Equation \eqref{AI-eq: agentBC} in the main paper, the following holds
		
		%
		\begin{equation*}
		\int (x_{t+1}^{i})^{2}\, \mathrm{d}\mathbb{P} \leq \int \left[((1+r(\mu_{t}))x_{t}^{i} +w(\mu_{t})e_{t}^{i})^{2} \right]^{2}\,\mathrm{d}\mathbb{P} < \infty 
		\end{equation*}
		
		Since $x_{0}^{i}$ has finite variance by assumption, $x_{1}^{i}$ will have finite variance. Moreover, if $x_{t}^{i}$ for any $t\in \mathbb{N}$ has finite variance, then  $x_{t+1}^{i}$ will have finite variance. By the principle of induction $x_{t}^{i}$ will have finite variance for all $t\in \mathbb{N}$. 
	\end{proof}
	
	\subsection*{Proofs for Section \ref{AI-sec: seqimprec}}
	
	For the next lemma, consider the setting of section \ref{AI-sec: seqprob} of the main paper.
	
	\begin{lemma}\label{eq: futureshock}
		
		
		Fix any $t\in\mathbb{N}$. If $(x_{t})_{t=0}^{\infty}$ and $(y_{t})_{t=0}^{\infty}$ are random variables adapted to $(\mathscr{F}_{t})_{t=0}^{\infty}$, then for any $j\geq t$, \[\mathbb{E}(y_{j+1}\vert \sigma(x_{t},e_{t},\dots,e_{j+1})) =\mathbb{E}(y_{j+1}\vert \sigma(x_{t},e_{t},\dots,e_{j}))\]
	\end{lemma}
	
	\begin{proof}
	
	Observe  $\mathbb{E}(y_{j+1}\vert \sigma(x_{t},e_{t},\dots,e_{j}))$ will be $\sigma(x_{t},e_{t},\dots,e_{j+1})$ measurable since $$\sigma(x_{t},e_{t},\dots,e_{j}))\subset\sigma(x_{t},e_{t},\dots,e_{j+1})$$ Thus, we will prove the lemma by using the definition of conditional expectations at section \ref{sec: probcond} of the  online appendix, and show
	
	\begin{equation}\label{eq: conddef}
	\int_{B} \mathbb{E}(y_{j+1}\vert \sigma(x_{t},e_{t},\dots,e_{j}))\, \mathrm{d}P = \int_{B} y_{j+1}\,\mathrm{d}P
	\end{equation}
	
	for all $B\in \sigma(x_{t},e_{t},\dots,e_{j+1})$. 
	
	We begin by verifying 
	%
	\begin{equation}\label{eq: firstsh}
	\mathbb{E}(y_{j+1}\vert \sigma(x_{t},e_{t},\dots,e_{j})) = \mathbb{E}(y_{j+1}\vert \sigma(x_{t},e_{t},\dots,e_{j},\eta_{j+1}))
	\end{equation}
	%
	By construction of the Markov process at Equation \eqref{eq: SRS} in the online appendix, $e_{i+1} = G(e_{i},\eta_{i+1})$ for each $i\geq 1$, where $\sigma(\eta_{i+1})$ is independent of $\sigma(x_{0},e_{0},\eta_{1},\dots,\eta_{i})$ and $G\colon E\times Z \rightarrow E$ is measurable. As such, each $e_{i}$ is a function  of the shocks $e_{0}$ and $\eta_{1},\dots, \eta_{i}$; applying the Doob-Dynkin Lemma (Fact \ref{fact: dd}), we have
	%
	\begin{equation*}
	\sigma(x_{t},e_{t},\dots,e_{j})\subset  \sigma(x_{0},e_{0},\eta_{1},\dots,\eta_{j})
	\end{equation*}
	
	It follows that since $\sigma(\eta_{j+1})$ and $\sigma(x_{0},e_{0},\eta_{1},\dots,\eta_{j})$ are independent, $\sigma(\eta_{j+1})$ and $\sigma(x_{t},e_{t},\dots,e_{j})$ will also be independent. 
	
	Now, use Fact \ref{clm: sigmaalg1} in the online appendix to write $$\sigma(x_{t})\cup\sigma(x_{t},e_{t},\dots,e_{j})\subset \sigma(x_{t},e_{t},\dots,e_{j})$$as such, 
	%
	\begin{equation*}
	\sigma(\sigma(x_{t}),\sigma(x_{t},e_{t},\dots,e_{j}))\subset \sigma(x_{t},e_{t},\dots,e_{j})
	\end{equation*}
	%
	Thus $\sigma(\eta_{j+1})$ and $\sigma(\sigma(x_{t}),\sigma(x_{t},e_{t},\dots,e_{j}))$ will be independent, since we showed above that $\sigma(\eta_{j+1})$ and $\sigma(x_{t},e_{t},\dots,e_{j})$ are independent. By Fact \ref{fact: indepex} in the online appendix, Equation \eqref{eq: firstsh} follows.
	
	Next, by the Doob-Dynkin Lemma, $\sigma(e_{j+1})\subset \sigma(e_{j},\eta_{j+1})$. As such, we can write the following inclusions
	%
	\begin{equation}\label{eq: inc3}
	\begin{aligned}
	\sigma(x_{t},e_{t},\dots,e_{j+1}) &\subset \sigma\left(\sigma(x_{t})\cup\sigma(e_{t})\cup\dots\cup \sigma(e_{j})\cup \sigma(e_{j},\eta_{j+1})\right)\\
	& \subset \sigma\left(\sigma(x_{t})\cup\sigma(e_{t})\cup\dots\cup \sigma(e_{j})\cup \sigma(e_{j})\cup\sigma(\eta_{j+1})\right)\\
	& = \sigma\left(\sigma(x_{t})\cup\sigma(e_{t})\cup\dots\cup \sigma(e_{j})\cup\sigma(\eta_{j+1})\right)
	\end{aligned}
	\end{equation}
	%
	where the second inclusion follows from Fact \ref{clm: sigmaalg1}. 
	
	To complete the proof by showing \eqref{eq: conddef}, let $B$ satisfy $B\in \sigma(x_{t},e_{t},\dots,e_{j+1})$. Recall Equation \eqref{eq: firstsh} and write 
	%
	\begin{multline*}
	\int_{B}\mathbb{E}(y_{j+1}\vert \sigma(x_{t},e_{t},\dots, e_{j}))\, \mathrm{d}P \\ = \int_{B}\mathbb{E}(y_{j+1}\vert \sigma(x_{t},e_{t},\dots, e_{j}, \eta_{j+1})\,\mathrm{d}P = \int_{B}y_{j+1}\,\mathrm{d}P
	\end{multline*}
	%
	where the final equality comes from the definition of conditional expectation and since, by \eqref{eq: inc3}, $B$ will satisfy $B\in\sigma(x_{t},e_{t},\dots,e_{j},\eta_{j+1})$.
	
	\end{proof}
	

  
	\begin{proof}[\textbf{Proof of Claim \ref{AI-clm: xtet}}]
	
	We will prove the claim by using the definition of conditional expectation from section \ref{sec: probcond} showing $\mathbb{E}(y_{t}\vert \sigma(x_{t-1}, e_{t-1}))$ is $\sigma(x_{t},e_{t})$ measurable and satisfies 
	%
	\begin{equation*}
	\int_{B} \mathbb{E}(y_{t}\vert \sigma(x_{t-1}, e_{t-1}))\,\mathrm{d}P = \int_{B}y_{t}\,\mathrm{d}P 
	\end{equation*}
	
	for $B\in \sigma(x_{t},e_{t})$.
	
	To show $\mathbb{E}(y_{t}\vert \sigma(x_{t-1},e_{t-1}))$ is $\sigma(x_{t},e_{t})$ measurable, observe $\mathbb{E}(y_{t}\vert \sigma(x_{t-1},e_{t-1}))$ can be written as a function of $\{x_{t},e_{t}\}$ as follows:
	%
	\begin{equation*}
	\{x_{t},e_{t}\} =  \{\mathbb{E}(y_{t}\vert \sigma(x_{t-1},e_{t-1})),e_{t}\} \mapsto \mathbb{E}(y_{t}\vert \sigma(x_{t-1},e_{t-1}))
	\end{equation*}
	%
	and thus measurability follows from the Doob-Dynkin Lemma (Fact \ref{fact: dd}).
	
	Next, by Lemma \ref{eq: futureshock}, we have
	%
	\begin{equation*}
	\mathbb{E}(y_{t}\vert \sigma(x_{t-1}, e_{t-1})) = \mathbb{E}(y_{t}\vert \sigma(x_{t-1}, e_{t-1}, e_{t}))
	\end{equation*}
	
	Moreover, $\sigma(x_{t}, e_{t})\subset \sigma(x_{t-1}, e_{t-1}, e_{t})$ by the Doob-Dynkin Lemma since $x_{t}$ is $\sigma(x_{t-1}, e_{t-1})$ measurable by definition of $x_{t}$. Now take any $B$ satisfying $B\in \sigma(x_{t}, e_{t})$. Since $B\in \sigma(x_{t-1}, e_{t-1}, e_{t})$, we can write  
	%
	\begin{equation*}
	\int_{B} \mathbb{E}(y_{t}\vert \sigma(x_{t-1}, e_{t-1}))\,\mathrm{d}P = \int_{B} \mathbb{E}(y_{t}\vert \sigma(x_{t-1}, e_{t-1}, e_{t}))\,\mathrm{d}P = \int_{B}y_{t}\,\mathrm{d}P 
	\end{equation*}
	
	as was to be shown to prove the claim. 
	
	\end{proof}

	\subsection*{Proofs for section \ref{AI-sec: diffc}}
	
	\begin{proof}[\textbf{Proof of Claim \ref{AI-clm: counter1}}]
	
	Define the set $C$, with $C\subset \mathbb{S}_{1}$, as follows:
	%
	\begin{equation*}
	C \colon = \{x\in \mathbb{S}_{1}\,\vert\, 0 \leq x \leq (1+\tilde{r}(x_{0}))x_{0} + \tilde{w}(x_{0})e_{0}\} = \Gamma_{0}(x_{0})
	\end{equation*}
	
	The set $C$ will be weakly closed since by Proposition \ref{AI-prop: appclosedgraph} in the main paper, $\Gamma_{0}$ has a closed graph, hence $\Gamma_{0}(x_{0})$ is weakly closed. To see $C$ is also weakly compact, note any $x\in C$ satisfies 
	%
	\begin{equation*}
	\Vert x\Vert \leq \Vert (1+\tilde{r}(x_{0}))x_{0} + \tilde{w}(x_{0})e_{0}\Vert\colon = \bar{M}
	\end{equation*}
	%
	where $\bar{M}<\infty$. Since $x_{0}$ and $e_{0}$ both have finite norm and $\tilde{r}(x_{0})$ is finite,  any $x\in C$ will satisfy $\Vert x \Vert \leq \bar{M}$. By Alaoglu's Theorem (see Example 6.3 in \cite{Andreu1991}), $C$ will be weakly compact since it is a weakly closed subset of a closed ball in $L^{2}(Z,P)$.
	
	
	Consider the set 
	%
	\begin{equation*}
	\Gamma_{1}(C)\colon =\cup_{x\in C}\Gamma_{1}(x) = \{y\,\vert\, y\in \Gamma_{1}(x),x\in \Gamma_{0}(x_{0})\}
	\end{equation*}
	
	We will construct a norm unbounded sequence in $\Gamma_{1}(C)$. First define a sequence $(x_{n})_{n=0}^{\infty}$ as $$x_{n}(x_{0},e_{0})\colon = x_{0}^{n}(1-x_{0}^{n})$$
	%
	Note $x_{n}\in C$ for each $n$ since $x_{0}^{n}(1-x_{0}^{n}) \leq 1 \leq w(x_{0})e_{0}$. Next, define $$y^{n}\colon = \frac{1}{2}(1+\tilde{r}(x_{n}))x_{n}$$
	
	Since $y_{n}\leq (1+\tilde{r}(x_{n}))x_{n}$ and $y_{n} \geq 0$, we have $y_{n}\in \Gamma_{1}(x_{n})$, thus $y_{n}\in \Gamma_{1}(C)$.
	
	Recall $F_{1}(K,L)= K^{\alpha -1}$ and use the definition of $L^{2}$ norm to write 
	%
	\begin{align*}
	\Vert y_{n}\Vert&  = \left(\int (y^{n})^{2}\,\mathrm{d}P\right)^{\frac{1}{2}}=\frac{1}{2}\left( \int[(1+\tilde{r}(x^{n}))x^{n}]^{2}\,\mathrm{d}P\right) ^{\frac{1}{2}}\\ & = \frac{1}{2}(\tilde{K}(x_{n})^{\alpha-1} +1 - \delta)\left(\int x_{0}^{2n}(1-x_{0}^{n})^{2}\,\mathrm{d}P\right) ^{\frac{1}{2}}\\
	& = \frac{1}{2}\left(\left(\int x_{0}^{n}(1-x_{0}^{n})\,\mathrm{d}P\right) ^{\alpha-1} +1 - \delta\right)\left(\int x_{0}^{2n}(1-x_{0}^{n})^{2}\,\mathrm{d}P\right) ^{\frac{1}{2}}\\
	& = \frac{1}{2}\left(\left(\int_{0}^{1} a^{n}(1-a^{n})\,\mathrm{d}a\right) ^{\alpha-1} +1 - \delta\right)\left(\int_{0}^{1} a^{2n}(1-a^{n})^{2}\,\mathrm{d}a\right) ^{\frac{1}{2}}\\
	& = \frac{1}{2}\left(\left(\frac{n}{1+3n+2n^{2}}\right)^{\alpha-1}+1 - \delta\right)\left(\frac{2n^{2}}{1+9n + 26n^{2}+ 24n^{3}}\right)^{\frac{1}{2}}\\
	& = \frac{1}{2}\frac{n\sqrt{2}\left[1 -\delta + n^{\alpha-1}((1 + n)(1 + 2n))^{1 - \alpha}\right]}{
		\sqrt{(1 + 2n)(1 + 3n)(1 + 4n)}}
	\end{align*}
	%
	
	The first and second lines follow from the definition of interest rates and aggregate capital given by Equation  \eqref{AI-eq: aggdefseq} in the main paper, along with the definition of $y_{n}$. The fourth line follows from the assumption that $x_{0}$ is uniformly distributed on the interval $[0,1]$. The fifth and sixth line are from algebra, solving out the definite integrals. 
	
	To conclude the proof, take the limit of the final line as $n$ converges to infinity to conclude  $\Vert y^{n}\Vert\rightarrow  \infty$. Since weakly compact sets must be norm bounded, the claim follows. 
	
	\end{proof}


\section{The Bellman Principle of Optimality}


This section presents the elementary Bellman Principle of Optimality theorems for a primitive form problem on arbitrary state-spaces. The results are not unique, however, the purpose here is to clarify the existence of optimal policy functions on arbitrary spaces. The primitive form problem consists of:

\begin{enumerate}
	\item  a state-space $\mathbb{X}$
	\item an action space $\mathbb{Y}$
	\item a feasibility correspondence $\Lambda\colon \mathbb{X} \rightarrow \mathbb{Y}$
	\item a transition function $\Phi \colon \Gr\Lambda \rightarrow \mathbb{X}$
	\item a per-period action pay-off $u\colon \Gr\Lambda \rightarrow \mathbb{R}$
	\item a discount fact $\beta\in (0,1)$
\end{enumerate}

Define the correspondence mapping a current state to sequences of feasible actions and states, $\mathcal{H}_{T}\colon \mathbb{X} \rightarrow (\mathbb{X}\times \mathbb{Y})^{\mathbb{N}}$ as follows:
%
\begin{equation*}
\mathcal{H}_{T}(x)\colon = \big\{(y_{t},x_{t})_{t=T}^{\infty}\,\big\vert\, y_{t}\in \Lambda(x_{t}), x_{t+1}= \Phi( x_{t},y_{t}), x_{T}=x, t\in \mathbb{N}, t \geq T\big\}
\end{equation*}
%

for any $x\in \mathbb{X}$. For $x\in \mathbb{X}$, define the primitive form value function:
%
\begin{equation}\label{eq: vfunc}
V(x) \colon = \sup_{(x_{t},y_{t})_{t=0}^{\infty}\in \mathcal{H}_{0}(x)}\quad\sum_{t=0}^{\infty}\beta^{t}u(x_{t},y_{t})
\end{equation}

and the primitive form Bellman Equation:
%
\begin{equation}\label{eq: Bellmaneq}
V(x)\colon = \sup_{y\in \Lambda(x)}\bigg\{u(x_{0},y)+\beta V(\Phi(x_{0},y))\bigg\}
\end{equation}


\begin{assumption}(\textbf{Growth Condition})\label{ass: growthcondbpo}
	For any $x\in \mathbb{X}$, there exists a sequence of real numbers, $(m_{t})_{t=0}^{\infty}$, such that for any $(x_{t},y_{t})_{t=0}^{\infty}\in\mathcal{H}(x)$, the following holds
	%
	\begin{equation}\label{eq: vafun}
	\vert u(x_{t},y_{t})\vert \leq m_{t}, \qquad \forall t\in \mathbb{N}
	\end{equation}
	%
	and 
	%
	\begin{equation}\label{eq: prim_bellman}
	\sum_{t=0}^{\infty}\beta^{t}m_{t} < \infty 
	\end{equation}
\end{assumption}


\begin{theorem}\label{eq: bpo1}
	Let Assumption \ref{ass: growthcondbpo} hold. If a function $V\colon \mathbb{X} \rightarrow \mathbb{R}$ is the value function defined by \eqref{eq: vfunc}, then $V$ satisfies the Bellman Equation \eqref{eq: Bellmaneq}. Conversely, if a function $V\colon \mathbb{X} \rightarrow \mathbb{R}$ satisfies the Bellman Equation \eqref{eq: Bellmaneq} and $\lim\limits_{t\rightarrow \infty}\beta^{t}V(x_{t})=0$, then $V$ is the value function defined by \eqref{eq: vfunc}.
\end{theorem}

\begin{proof}
	Let $V$ be a value the function defined by \eqref{eq: vfunc}. For any $x_{0}\in \mathbb{X}$, by definition, the function $V$ will satisfy
	%
	\begin{align*}
	V(x_{0})&  = \sup_{(x_{t}, y_{t})_{t=0}^{\infty}\in \mathcal{H}_{0}(x_{0})} \sum_{t=0}^{\infty}\beta^{t}u(x_{t},y_{t})\\& = \sup_{(x_{t},y_{t})_{t=0}^{\infty}\in \mathcal{H}_{0}(x_{0})}\left\{ u(x_{0},y_{0}) + \sum_{t=1}^{\infty}\beta^{t}u(x_{t},y_{t})\right\}
	\\&  = \sup_{y_{0}\in \Lambda(x_{0}),x_{0}}\,\sup_{(x_{t}, y_{t})_{t=1}^{\infty}\in \mathcal{H}_{1}(\Phi(x_{0},y_{0}))}\left\{ u(x_{0},y_{0}) + \sum_{t=1}^{\infty}\beta^{t}u(x_{t},y_{t})\right\}\\
	& = \sup_{y_{0}\in \Lambda(x_{0})} \left\{ u(x_{0},y_{0})  + \sup_{(x_{t}, y_{t})_{t=1}^{\infty}\in \mathcal{H}_{1}(\Phi(x_{0},y_{0}))} \sum_{t=1}^{\infty}\beta^{t}u(x_{t},y_{t})\right\}\\ 
	&= \sup_{y_{0}\in \Lambda(x_{0})} \bigg\{u(x_{0},y_{0}) + \beta V(\Phi(x_{0},y_{0}))\bigg\}
	\end{align*}
	
	
	The second equality is a simple expansion of the infinite sum. The third equality follow from Lemma 1 by \cite{Kamihigashi2008}, which confirms we can split the supremum over into two suprema. The final equality holds from the definition of the value function. 
	
	Next, suppose $V$ satisfies the Bellman Equation, Equation \eqref{eq: Bellmaneq}. Let $(x_{t},y_{t})_{t=0}^{\infty}$ be any sequence satisfying $(x_{t},y_{t})_{t=0}^{\infty}\in \mathcal{H}_{0}(x_{0})$. Note 
	
	\begin{align*}
	V(x_{0})& = \sup_{y\in \Gamma(x)} \{u(x_{0},y) + \beta V(\Phi(x_{0},y))\}\\
	& \geq u(x_{0},y_{0}) + \beta V(\Phi(x_{0},y_{0})) \\
	& \geq u(x_{0},y_{0}) + \beta u(x_{1},y_{1}) +\beta^{2}V(\Phi(x_{1},y_{1}))
	\end{align*} 
	
	In particular, for any $T$, 
	%
	\begin{align*}
	V(x_{0})& = \sup_{y\in \Gamma(x)} \{u(x_{0},y_{0}) + \beta V(\Phi(x_{0},y_{0}))\}\\
	& \geq \sum_{t=0}^{T-1}\beta^{t}u(x_{t},y_{t}) + \beta^{T} V(x_{T})
	\end{align*} 
	
	By Assumption \ref{ass: growthcondbpo}, $\vert u(x_{t},y_{t})\vert \leq m_{t}$ where $\sum_{t=0}^{\infty}m_{t} <\infty$. We thus satisfy the requirements of the dominated convergence theorem and conclude
	%
	\begin{align*}
	V(x_{0})&  \geq \lim_{T\rightarrow \infty}\left\{\sum_{t=0}^{T-1}\beta^{t}u(x_{t},y_{t}) + \beta^{T} V(x_{T})\right\}\\
	& = \lim_{T\rightarrow\infty}\sum_{t=0}^{T-1}\beta^{t}u(x_{t},y_{t}) + \lim_{T\rightarrow\infty}V(x_{T}) \\
	&= \sum_{t=0}^{\infty}\beta^{t}u(x_{t},y_{t})
	\end{align*}
	
	%
	where the third line uses the assumption $\lim\limits_{t\rightarrow \infty}V(x_{t}) = 0$. Since $(x_{t},y_{t})_{t=0}^{\infty}\in \mathcal{H}(x_{0})$ was arbitrary, $V(x_{0})$ must satisfy \eqref{eq: vfunc}, completing the proof. 
	
\end{proof}


\begin{theorem}\label{thm: bpo_prim}
	Let Assumption \ref{ass: growthcondbpo} hold and fix  $x_{0}\in \mathbb{X}$. If a sequence $(x_{t},y_{t})_{t=0}^{\infty}\in \mathcal{H}(x_{0})$ achieves the value function 
	%
	\begin{equation}
	V(x_{0}) = \sum_{t=0}^{\infty}\beta^{t}u(x_{t},y_{t})
	\end{equation}
	%
	then 
	%
	\begin{equation}\label{eq:BE_seq}
	V(x_{t}) = u(x_{t},y_{t})  +\beta V(\Phi(x_{t}, y_{t})), \qquad\qquad \forall t\in \mathbb{N}
	\end{equation}
	
	Conversely, if any sequence $(x_{t},y_{t})_{t=0}^{\infty}\in \mathcal{H}_{0}(x_{0})$ and function $V\colon \mathbb{X}\rightarrow \mathbb{R}$ satisfies \eqref{eq:BE_seq} and $\beta^{t}V(x_{t})\rightarrow 0$, then $(x_{t},y_{t})_{t=0}^{\infty}$ achieves the value function at $x$. 
\end{theorem}

\begin{proof}
	Let the sequence $(x_{t},y_{t})_{t=0}^{\infty}$ achieve the value function. We will proceed by induction. Let $t=0$, for any sequence $(\tilde{x}_{t}, \tilde{y}_{t})_{t=0}^{\infty}\in \mathcal{H}_{0}(x_{0})$, we have
	%
	\begin{multline*}
	V(x_{0}) = u(x_{0}, y_{0}) + \beta U((x_{t},y_{t})_{t=1}^{\infty}))\\  \geq u(x_{0}, \tilde{y}_{0})  +  \beta U((\tilde{x}_{t},\tilde{y}_{t})_{t=1}^{\infty})) 
	\end{multline*}
	
	where  we define $U((\tilde{x}_{t},\tilde{y}_{t})_{t=T}^{\infty})) \colon =  \sum_{t=T}^{\infty}\beta^{t-T}u(\tilde{x}_{t},\tilde{y}_{t})$ for any $T\in \mathbb{N}$. 
	
	In particular, for any $(\tilde{x}_{t}, \tilde{y}_{t})_{t=0}^{\infty}\in \mathcal{H}_{0}(x_{0})$ such that $\tilde{y}_{0} = y_{0}$, we have
	%
	\begin{equation*}
	u(x_{0}, y_{0}) + \beta U((x_{t},y_{t})_{t=1}^{\infty})) \geq u(x_{0}, y_{0})  +  \beta U((\tilde{x}_{t},\tilde{y}_{t})_{t=1}^{\infty})) 
	\end{equation*}
	%
	implying 
	%
	\begin{equation*}
	 U((x_{t},y_{t})_{t=1}^{\infty})) \geq   U((\tilde{x}_{t},\tilde{y}_{t})_{t=1}^{\infty})) 
	\end{equation*}
	
	for any $(\tilde{x}_{t},\tilde{y}_{t})_{t=1}^{\infty}\in \mathcal{H}_{1}(\Phi(x_{0},y_{0}))$. This gives $U((x_{t},y_{t})_{t=1}^{\infty}) = V(\Phi(x_{0},y_{0}))$ and allows us to conclude \eqref{eq:BE_seq} holds for $t=0$. 
	
	Now make the inductive assumption and let \eqref{eq:BE_seq} hold for any $T-1$. We have 
	%
	\begin{equation*}
	V(x_{T}) = u(x_{T}, y_{T}) + \beta U((x_{t},y_{t})_{t=T+1}^{\infty}) \geq u(x_{T}, \tilde{y}_{T})  +  \beta U((\tilde{x}_{t},\tilde{y}_{t})_{t=T+1}^{\infty}) 
	\end{equation*}
	%
	holds for any $(\tilde{x}_{t}, \tilde{y}_{t})_{t= T}^{\infty}\in \mathcal{H}_{T}(x_{T})$. In particular, for any $(\tilde{x}_{t}, \tilde{y}_{t})_{t=T}^{\infty}\in \mathcal{H}_{T}(x_{T})$ such that $\tilde{y}_{T} = y_{T}$, we have
	%
	\begin{equation*}
	u(x_{T},y_{T}) + \beta U((x_{t},y_{t})_{t=T+1}^{\infty}) \geq u(x_{T}, y_{T}) + \beta U((\tilde{x}_{t},\tilde{y}_{t})_{t=T+1}^{\infty})
	\end{equation*}
	%
	implying
	%
	\begin{equation*}
		U((x_{t},y_{t})_{t=T+1}^{\infty}) \geq  U((\tilde{x}_{t},\tilde{y}_{t})_{t=T+1}^{\infty})
	\end{equation*}
	%
	for any $(\tilde{x}_{t},\tilde{y}_{t})_{t=T+1}^{\infty} \in \mathcal{H}_{T+1}(\Phi(x_{t},y_{t}))$. As such, $V(\Phi(x_{t},y_{t})) = U((x_{t},y_{t})_{t=T+1}^{\infty})$, allowing us to conclude \eqref{eq:BE_seq} holds for all $t\in \mathbb{N}$. 
	
	Now suppose $(x_{t},y_{t})$ with $(x_{t},y_{t})_{t=0}^{\infty}\in \mathcal{H}_{0}(x_{0})$ and a function $V\colon \mathbb{X}\rightarrow \mathbb{R}$ satisfies \eqref{eq:BE_seq} and $\beta^{t}V(x_{t})\rightarrow 0$. By Theorem \eqref{eq: bpo1}, $V$ will be the value function and
	%
	\begin{equation*}
	V(x_{0}) \colon = \sup_{(x_{t},y_{t})_{t=0}^{\infty}\in \mathcal{H}_{0}(x)}\quad\sum_{t=0}^{\infty}\beta^{t}u(x_{t},y_{t})
	\end{equation*}
	
	Since $V$ and $(x_{t},y_{t})_{t=0}^{\infty}$ satisfies the Bellman Equation, note
	%
	\begin{align*}
	V(x_{0})&  = u(x_{0},y_{0}) + \beta V(x_{1})\\
	& = u(x_{0},y_{0}) + \beta u(x_{1}, y_{1}) + \beta^{2}V(x_{2})
	\end{align*}
	%
	moreover, 
	%
	\begin{align*}
	V(x_{0})&  = \sum_{t=0}^{T-1}\beta^{t}u(x_{t},y_{t}) + \beta^{T} V(x_{T})
	\end{align*}
	
	By Assumption \ref{ass: growthcondbpo}, $\vert u(x_{t},y_{t})\vert \leq m_{t}$ where $\sum_{t=0}^{\infty}m_{t} <\infty$. We satisfy the requirements of Dominated Convergence Theorem and conclude
	%
	\begin{align*}
	V(x_{0})&  = \lim\limits_{T\rightarrow \infty }\sum_{t=0}^{T-1}\beta^{t}u(x_{t},y_{t}) +  \lim\limits_{T\rightarrow \infty }\beta^{T} V(x_{T}) =\sum_{t=0}^{\infty}\beta^{t}u(x_{t},y_{t})
	\end{align*}
	
	Thus $(x_{t},y_{t})_{t=0}^{\infty}$ achieves the value function as was to be shown. 
	\end{proof}

	
	Define the optimal policy correspondence $H\colon \mathbb{X}\rightarrow \mathbb{Y}$ as
	%
	\begin{equation}\label{eq: optpolcorr}
	H(x) = \argmax_{y\in \Lambda(x)}\big\{ u(x,y) + \beta V(\Phi(x,y))\big\}
	\end{equation}

\begin{corollary}\label{corr: polext}
	Suppose Assumption \ref{ass: growthcondbpo} holds.  If the value function $V(x)$ is achieved by an optimal sequence for each $x$, then 
	\begin{enumerate}
		\item there exists an optimal policy correspondence $H$ defined by \eqref{eq: optpolcorr}
		\item there exists an optimal policy function $h\colon \mathbb{X}\rightarrow \mathbb{Y}$ such that $h(x)\in H(x)$ for each $x$ 
		\item for any $x\in \mathbb{X}$, the sequence $(x_{t}, h(x_{t}))_{t=0}^{\infty}$, where $x_{t+1} = \Phi(x_{t},h(x_{t}))$ and $x_{0}=x$, achieves the value function.
	\end{enumerate}
\end{corollary}

\begin{proof}
	If the value function $V(x)$ is achieved by an optimal sequence for each $x$, then
	
	\begin{equation*}
	V(x) \colon = \sup_{y\in \Lambda(x)}\big\{u(x,y)+ \beta V(\Phi(x,y))\big\} = u(x,y_{0})+ \beta V(\Phi(x,y_{0}))
	\end{equation*}
	
	% 
	for $y_{0}\in \Lambda(x)$. Since the above holds for any $x\in \mathbb{X}$, $H(x)$ will be non-empty for each $x$. By the axiom of choice, there exists a function $h\colon \mathbb{X}\rightarrow \mathbb{Y}$ such that $h(x)\in H(x)$ for each $x$ (see section 17.11 in \cite{Aliprantis2005}).
	
	Finally, fix $x\in \mathbb{X}$. To show 3., for each $t$, we have
	%
	\begin{multline*}
	V(x_{t})  \colon = \sup_{y\in \Lambda(x_{t})}\bigg\{ u(x_{t},y) + \beta V(\Phi(x_{t},y)) \bigg\} \\ = u(x_{t},h(x_{t})) + \beta V(\Phi(x_{t},h(x_{t})))
	\end{multline*}
	%
	establishing, by Theorem \ref{thm: bpo_prim}, that $(x_{t}, h(x_{t}))_{t=0}^{\infty}$ achieves the value function  $V(x)$. 
	
\end{proof}


\section{Further Discussion on Dynamic Programming Limitations}

	\subsubsection{Non-Compactness of The Feasibility Correspondence}
	
	I  consider the following topologies:
	\begin{enumerate}
		\item The weak topology if we let $\mathbb{Y} = L^{2}(S, \lambda)$, where $\lambda$ is the Lebesgue measure (weakly closed and norm-bounded sub-sets are  compact)
		\item The weak topology if we let  $\mathbb{Y} = L^{1}(S, \lambda)$ (order intervals are weakly compact)
		\item The weak-star topology if we let $\mathbb{Y} =L^{\infty}(S, \lambda)$ (weak-star closed and norm-bounded sub-sets will are compact)
		\item The weak topology if we let $\mathbb{Y} =\mathscr{C}b(S, \lambda)$, the space of continuous bounded functions on $S$
	\end{enumerate}
	
	Consider the weak topology on $L^{2}(S,\lambda)$. If we let $\mathbb{M}$ be the space of Borel probability measures on $S$, then $\Phi$ will not be defined. To see why, specialise to the case where shocks are {\sc iid} and take on discrete values $\{e_{1},e_{2}\}$ with probability $\pi_{1}$ and $\pi_{2}$. Since the endowment shocks are independent of agents' previous shocks and current assets, we can track the state by tracking the marginal distribution on $A$ at each $t$, $\tilde{\mu}_{t}$. The marginal distribution evolves according to
	%
	\begin{equation*}
	\tilde{\mu}_{t+1}(B) = \sum _{j\in \{1,2\}}\pi_{j}\int \mathbb{1}_{B}\{h(a, e_{j})\} \tilde{\mu}_{t}(da), \qquad B\in \mathscr{B}(A)
	\end{equation*}
	
	Now suppose $\tilde{\mu_{t}} = \delta_{x}$ is the Dirac delta measure which puts all weight on a point $x\in A$. We write 
	%
	\[
	\tilde{\mu}_{t+1}(B) = \sum _{j\in \{1,2\}}\pi_{j} \mathbb{1}_{B}\{h(x, e_{j})\}, \qquad\qquad B\in \mathscr{B}(A)\]
	
	Recalling  $h(\cdot,e_{j})$ satisfies $h(\cdot,e_{j})\in L^{2}(S,\lambda)$,  $h(\cdot,e_{j})$ is an equivalence class of functions equal $\lambda$ -almost everywhere; $\mu_{t+1}$ as defined above will not be a measure on $\mathscr{B}(\mathbb{R})$ because the evaluation
	%
	\begin{align*}
	\mathbb{1}_{B} = 
	\begin{cases}
		 1 &  \text{if}\,\,\,\, h(x,e_{j})\in (0,1)\\
		0&  \text{if}\,\,\,\, h(x,e_{j})\not\in (0,1)\\
	\end{cases}
	\end{align*}
	%
	
	is not defined. In particular, let $h^{\prime}$ and $h^{\prime\prime}$ be two functions belonging to the equivalence class $h$. Since the functions can differ on measure zero sets, we can have $h^{\prime}(x,e_{j})\in (0,1)$ but $h^{\prime\prime}(x,e_{j})\not\in (0,1)$.
	
	
	We could take $\mathbb{M}$ to be the space of absolutely continuous measures on $S$. However, in this case, take $h$ to be a constant function, then $\mu\circ h^{-1}$ will be the Dirac delta function, which is not absolutely continuous. The operator $\Phi$ will then map to values outside of $\mathbb{M}$.
	
	Similar problems arise if we consider $\mathbb{Y} = L^{1}(S, \lambda)$ and $\mathbb{Y} = L^{\infty}(S, \lambda)$. 
	
	Finally, the space of continuous bounded real functions on $S$ with the weak topology does not present useful compact sets: unit balls will not be weakly compact since the space is not reflexive and order intervals are only compact if the dual pairing is a symmetric Reisz pair (see section 8.16 in \cite{Aliprantis2005}).
	
	\subsubsection{Discussion of Standard Dynamic Optimisation Theory}\label{sec: stand}
		   
	 Standard theory uses two approaches can be used to verify existence in an infinite horizon dynamic optimisation problem: dynamic programming and product topology approaches. Both these approaches require the feasibility correspondences to have compact image sets, and as discussed in  section \ref{AI-sec: pathos1} of the main paper, the constrained planner's sequential and recursive problems will not have compact image sets. I briefly discuss why standard theory requires compact image sets below. 
	 
	 Write the Bellman Operator, $T$, for the recursive constrained planner's problem as
	 %
	 \begin{equation}\label{eq: bellmanrec}
	 TV^{\prime}(\mu) \colon = \sup_{h\in \Lambda(\mu) } \big\{ u(\mu,h) + \beta V^{\prime}(\Phi(\mu,h))\big\}
	 \end{equation}
	 %
	 where $V^{\prime}$ is an extended real-valued function on $\mathbb{M}$. 
	 
	 Recall the standard dynamic programming procedure shows a fixed point to the Bellman Operator is the value function, $V$. If $V$ is (semi) continuous and the feasibility correspondence compact valued, we can confirm existence of an optimal policy. (Semi) continuity of $V$ is usually achieved by showing the Bellman Operator maps (semi) continuous functions to (semi) continuous functions,\footnote{We can either show the Bellman Operator maps a space of bounded continuous functions to bounded continuous functions, which is complete, or use more general results, say by \cite{Kamihigashi2014a} section 3.2, to show the limit of Bellman iteration can preserve (semi) continuity.} which allows us to show sequences of iterations on the Bellman Operator converge to a (semi) continuous fixed point. To show the Bellman Operator preserves (semi) continuity, the standard approach (see \cite{Stachurski2009}, Appendix B and \cite{Stokey1989}, chapter 4) is to use Berge's Theorem, which requires $\Lambda$ to be upper hemicontinuous and compact-valued (\cite{Aliprantis2005} Lemma 17.30).  And compact-valued upper hemicontinuous correspondences have compact image sets (\cite{Aliprantis2005} Lemma 17.8). 
	 
	 I discuss the product space approach using a reduced form stationary problem for easier notation, though similar ideas can work on the primitive form problem. Let $\mathbb{S}$, $\rho$ and $\Gamma$ be the state space, the feasibility correspondence and pay-offs for each $t$. The approach (see \cite{LeVan2002a} Theorem 1, \cite{Acemoglu2009} Theorem 6.3, or \cite{Kamihigashi2017} Proposition 6.1), assuming $S$ is a metric space, works by showing the function $(x_{t})_{t=0}^{\infty}\mapsto \sum_{t=0}^{\infty}\rho(x_{t},x_{t+1})$ is upper semicontinuous on a compact space of feasible sequences $\mathcal{G}(x_{0})$. To show $\mathcal{G}(x_{0})$ is compact, we assume $\Gamma$ is upper hemicontinuous and compact-valued, hence $\Pi_{t=0}^{\infty}\Gamma^{t}(x_{0})$ is sequentially compact in the product topology. Since $\Gamma$ and will also have closed graph (see Theorem 17.10 in \cite{Aliprantis2005}), and $\mathcal{G}(x_{0})\subset \Pi_{t=0}^{\infty}\Gamma^{t}(x_{0})$, $\mathcal{G}(x_{0})$ will be compact.
	 
	 
	 
	 %We may be able to find a fixed point $V$ to the Bellman Operator at \eqref{eq: bellmanrec}, even without making topological assumptions on $\mathbb{M}$ or $\mathbb{Y}$ by using new results in \cite{Kamihigashi2014a}.\footnote{\cite{Kamihigashi2014a} gives results for a reduced form problem, though the arguments could be extended to incorporate primitive form problems such as the recursive planner's problem} However, we still face the challenge of showing existence of an optimal policy; to replace $\text{sup}$ with $\text{max}$ in the Bellman Operator, we need (semi) continuity on $V$ and we require $\Lambda$ to be compact valued. 
	 
	 %We then require \emph{joint} semi-continuity of the map
	 %
	 %\[ \{\mu,h\} \mapsto u(\mu,h) + \beta V^{\prime}(\Phi(\mu, h))\]
	 %=
	 %in addition to a compact valued and upper-hemicontinuous $\Lambda$.  


 	
 	\bibliographystyle{apalike}
 	
 	\bibliography{thesis}
 	

\end{document}
